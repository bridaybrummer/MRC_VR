---
title: "Confidence Interval Modelling Methods"
description: "Methods for constructing confidence and prediction intervals for mortality rate estimates"
date: "2026-01-20"
categories: [methods, statistics, negative-binomial, Monte-Carlo]
execute:
  echo: false
  warning: false
  message: false
---

## Overview

This document describes how we estimated expected weekly deaths and mortality rates using a negative-binomial generalized linear model, and how we constructed confidence or prediction intervals using: (i) a simple Poisson approximation, (ii) rate-based intervals on the log scale, and (iii) a Monte Carlo simulation based on the fitted negative-binomial model.

Throughout, let:

- $t$ index epidemiological weeks,
- $Y_t$ denote the observed number of deaths in week $t$,
- $\mu_t = \mathbb{E}[Y_t]$ denote the expected number of deaths in week $t$,
- $\theta$ denote the negative-binomial dispersion parameter,
- $\text{pop}_t$ denote the population at risk in week $t$.

We separate the data into:

- **Baseline period:** 2015–2019, used for model fitting.
- **Projection period:** 2020–2021, for which we generate counterfactual predictions under a “no COVID-19” scenario.

## Baseline model: negative-binomial GLM

We model weekly counts using a negative-binomial GLM with log link:

$$
Y_t \mid \mu_t, \theta \sim \text{NegBin}(\mu_t, \theta),
$$

where $\mu_t$ is linked to covariates via

$$
\log(\mu_t) = \beta_0 + f_{\text{week}}(\text{week}_t) + \beta_{\text{time}} \, t + \log(\text{pop}_t).
$$

Here:

- $f_{\text{week}}(\cdot)$ is a seasonal effect modelled using a factor for ISO week ($\text{week}_t$ taking values 1 to 52),
- $t$ is a linear time index, capturing gradual trend,
- $\log(\text{pop}_t)$ enters the model as an **offset**, so $\mu_t$ scales with the population at risk.

In practice we fit:

$$
\log(\mu_t) = \beta_0 + \sum_{k=2}^{K} \beta_k^{\text{week}} \, \mathbb{I}(\text{week}_t = k) + \beta_{\text{time}} \, t + \log(\text{pop}_t),
$$

with $K$ week categories (week 1 as reference). Model parameters $\beta$ and $\theta$ are estimated on the **baseline** period (2015–2019) using maximum likelihood via `glm.nb`.

For the **projection** period (2020–2021), we plug observed covariates ($\text{week}_t, t, \text{pop}_t$) into the fitted model to obtain:

- Linear predictor: $\hat{\eta}_t = \hat{\beta}_0 + \hat{f}_{\text{week}}(\text{week}_t) + \hat{\beta}_{\text{time}} \, t + \log(\text{pop}_t)$,
- Expected counts: $\hat{\mu}_t = \exp(\hat{\eta}_t)$.

These $\hat{\mu}_t$ represent **counterfactual expected deaths** in the absence of the pandemic.

## Basic Poisson confidence intervals for predicted counts

As a simple baseline, we approximate the predictive distribution of counts using a Poisson variance:

$$
\text{Var}(Y_t) \approx \mu_t.
$$

Replacing $\mu_t$ with $\hat{\mu}_t$, the standard deviation is:

$$
\widehat{\text{SD}}(Y_t) \approx \sqrt{\hat{\mu}_t}.
$$

A **Wald-type** 95% interval for the expected count at week $t$ is then:

$$
\hat{\mu}_t \pm 1.96 \, \sqrt{\hat{\mu}_t}.
$$

These intervals:

- Are simple to compute,
- Treat the variance as if the process were Poisson,
- Do **not** explicitly account for over-dispersion or parameter uncertainty, but provide a useful “first approximation” for visualisation and comparison.

## Mortality rates and log-scale confidence intervals

To interpret results on a population-standardised scale, we define the weekly **mortality rate** as:

$$
r_t = \frac{\mu_t}{\text{pop}_t} \quad \text{(deaths per person per week)}.
$$

Using the Poisson approximation again, we have:

$$
\text{Var}(\log r_t) \approx \text{Var}(\log \mu_t - \log \text{pop}_t) \approx \text{Var}(\log \mu_t) \approx \frac{1}{\mu_t}.
$$

Thus, an approximate standard error for the log-rate is:

$$
\text{SE}(\log \hat{r}_t) \approx \sqrt{\frac{1}{\hat{\mu}_t}}.
$$

We then construct a 95% interval on the **log-rate** scale:

$$
\log \hat{r}_t \pm 1.96 \, \text{SE}(\log \hat{r}_t),
$$

and exponentiate to obtain a 95% confidence interval for $r_t$:

$$
\bigl[
\hat{r}_t^{\text{lo}},\, \hat{r}_t^{\text{hi}}
\bigr]= \left[
\exp\bigl(\log \hat{r}_t - 1.96 \, \text{SE}(\log \hat{r}_t)\bigr),
\;
\exp\bigl(\log \hat{r}_t + 1.96 \, \text{SE}(\log \hat{r}_t)\bigr)
\right].
$$

For reporting, we often scale to **deaths per 100 000 population**:

$$
r_{t,\,100k} = r_t \times 100\,000,
$$

and apply the same multiplicative factor to the confidence bounds.

This approach:

- Naturally constrains rates to be positive,
- Provides interpretable intervals on a rate scale,
- Still relies on a Poisson-based variance approximation, but now for rates rather than counts.

## Monte Carlo simulation from the negative-binomial model

To more fully account for **over-dispersion** and **parameter uncertainty**, we perform a Monte Carlo simulation based on the fitted negative-binomial model.

### Step 1: Parameter uncertainty in $\beta$

Let $\hat{\beta}$ denote the vector of estimated regression coefficients and $\widehat{\Sigma}$ their estimated variance–covariance matrix from the fitted `glm.nb` model. We approximate

$$
\beta \mid \text{data} \approx \mathcal{N}(\hat{\beta}, \widehat{\Sigma}).
$$

We draw $S$ Monte Carlo samples:

$$
\beta^{(s)} \sim \mathcal{N}(\hat{\beta}, \widehat{\Sigma}), \quad s = 1,\dots,S.
$$

### Step 2: Compute linear predictors and means

For each draw $s$ and each week $t$ in the projection period, we compute the linear predictor:

$$
\eta_t^{(s)} = x_t^\top \beta^{(s)} + \log(\text{pop}_t),
$$

where $x_t$ is the vector of covariates (intercept, week indicators, time). We then transform to the mean scale via the log link:

$$
\mu_t^{(s)} = \exp(\eta_t^{(s)}).
$$

### Step 3: Predictive counts under the negative-binomial model

Using the fitted dispersion $\hat{\theta}$ from the baseline model, we generate predictive counts for each draw:

$$
Y_t^{(s)} \sim \text{NegBin}(\mu_t^{(s)}, \hat{\theta}).
$$

This step captures:

- The uncertainty in the expected value $\mu_t$ due to uncertainty in $\beta$,
- The **extra-Poisson** variability implied by the negative-binomial distribution.

### Step 4: Construct prediction intervals

For each week $t$, we now have a Monte Carlo sample $\{Y_t^{(1)}, \dots, Y_t^{(S)}\}$. We define pointwise 95% prediction intervals as the 2.5th and 97.5th percentiles:

$$
\bigl[ \tilde{y}_t^{\text{lo}},\; \tilde{y}_t^{\text{hi}} \bigr]= \left[
\text{quantile}\bigl(\{Y_t^{(s)}\}_{s=1}^S, 0.025\bigr),
\;
\text{quantile}\bigl(\{Y_t^{(s)}\}_{s=1}^S, 0.975\bigr)
\right],
$$

with a central summary such as the median:

$$
\tilde{y}_t^{\text{med}} = \text{quantile}\bigl(\{Y_t^{(s)}\}_{s=1}^S, 0.5\bigr).
$$

Analogously, we obtain Monte Carlo intervals for **mortality rates** by transforming:

$$
r_t^{(s)} = \frac{\mu_t^{(s)}}{\text{pop}_t} \quad\text{or}\quad r_t^{(s)} = \frac{Y_t^{(s)}}{\text{pop}_t},
$$

and summarising the empirical distribution of $\{r_t^{(s)}\}$ using the same percentile-based intervals.

### Interpretation

This Monte Carlo approach:

- Propagates uncertainty in the regression parameters $\beta$,
- Incorporates over-dispersion through $\hat{\theta}$,
- Produces **prediction intervals** for future counts (or rates), rather than merely confidence intervals for the mean.

Compared with the simpler Poisson- or rate-based intervals, these prediction intervals are typically wider and better reflect the variability observed in real-world mortality data.

## Summary of the CI “ladder”

We therefore obtain a progression from simple to more advanced uncertainty quantification:

1. **Poisson Wald intervals for counts**
   $$
   \hat{\mu}_t \pm 1.96 \sqrt{\hat{\mu}_t},
   $$
   assuming Poisson variance and ignoring parameter uncertainty.

2. **Log-scale intervals for mortality rates**
   $$
   \log \hat{r}_t \pm 1.96 \sqrt{1 / \hat{\mu}_t},
   $$
   exponentiated to give positive rate intervals (e.g., per 100 000).

3. **Monte Carlo prediction intervals from the negative-binomial GLM**

   - $\beta^{(s)} \sim \mathcal{N}(\hat{\beta}, \widehat{\Sigma})$,
   - $Y_t^{(s)} \sim \text{NegBin}(\mu_t^{(s)}, \hat{\theta})$,
   - Quantile-based intervals from the empirical distributions.

This “CI ladder” allows us to start with fast, interpretable approximations and move towards a more realistic prediction framework that accounts for both model and process uncertainty.

